ITERATIVE IMPROVEMENT

-----
Hill Climbing

	Iteratively maximiaze the "value" of current state, by replacing it by successor state that has highest value, as long as possible

function hill-climbing(problem) return a solution state
	input:		problem
	local var:	current, a current node
			next, a next node
	
	current <- make-node(initial-state(problem))
	while true:
		next <- a highest valued successor of current
		if value(next) < value(current) then return current
		curr <- next
	end

-----
SA (Simulated Annealing)

function Simulated-Annealing return a solution state
	input:		problem
			schedule: a mapping from time to temperature
	local var:	current, a node
			next, anode
			T, a temperature var

	current <- make-node(initial-state(problem))
	for t <- 1 to infinity:
		T <- schedule(t)
		if T = 0 then return current
		next <- random successor of current 
		delta_E = value(next) - value(current)
		if delta_E>0 then current <- next;
		else current <- next only with probability p = e^(delta_E/T)


	
	Completeness: yes, if T converge to 0
	Optimal: yes if T decrease slower than 1/log(n)

-----
Genetic Algorithm: to find a solutio in large complex space

	1. Represent a solution like a DNA sequence
	2. Rank the population by their fitness - we need to be able to measure fitness in polynomial time, otherwise we are in trouble
	3. Breed the population (combine the better fitness ones)
		- crossover: exchanging information thru some part of information 
		- better fitness people should mate more often and poor fitness people should mate less often
		- not all crossed pairs are viable and different GA problems may have different bounds
		- many GA problems need special crossover rules 
		- to get over local extrema, we need to introduce random effect: Mutation, Cataclysm, Annealing of Mating Pairs


	GA is good if your space is loaded with lots of weird bumps and local minima; if dont understand the process of the problem space; if you have a lot of processors

